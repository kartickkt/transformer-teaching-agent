[
  {
    "source": "tensor2tensor",
    "target": "Transformer architecture",
    "relation": "prerequisite"
  },
  {
    "source": "tensor2tensor",
    "target": "encoder self-attention",
    "relation": "prerequisite"
  },
  {
    "source": "tensor2tensor",
    "target": "total computational complexity per layer",
    "relation": "prerequisite"
  },
  {
    "source": "tensor2tensor",
    "target": "TFLOPS",
    "relation": "prerequisite"
  },
  {
    "source": "tensor2tensor",
    "target": "large training data",
    "relation": "prerequisite"
  }
]